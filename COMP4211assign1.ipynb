{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assign1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "atreec",
      "language": "python",
      "name": "atreec"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/yueransutd/Notes/blob/master/COMP4211assign1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YEI9qpLRxRtC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the function below to fetch the digits dataset"
      ]
    },
    {
      "metadata": {
        "id": "6Gv1w4YOxRtE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "def get_digits_dataset():\n",
        "    '''\n",
        "    Use this function to collect your training and testing data. This function\n",
        "    uses the first 80% of the dataset as training data and the last 20% as test data.\n",
        "    \n",
        "    Returns\n",
        "    ----------\n",
        "    X_train : (number train samples, number features)\n",
        "    y_train : (number train samples, )\n",
        "    X_test : (number test samples, number features)\n",
        "    y_test : (number test samples,)\n",
        "    '''\n",
        "    per_train_sz = 0.8\n",
        "    digits = datasets.load_digits()\n",
        "    X = digits.images.reshape([digits.images.shape[0], -1])\n",
        "    y = digits.target\n",
        "    train_id = int(X.shape[0] * per_train_sz)\n",
        "    X_train, y_train, X_test, y_test = X[:train_id], y[:train_id], X[train_id:], y[train_id:]\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wLD0LPHcxRtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc170e00-9dfe-48e3-98ac-e0782f55e93e"
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test= get_digits_dataset()\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((1437, 64), (1437,), (360, 64), (360,))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ICjpFnzMbNSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "f155b84c-4918-467d-f776-cb55339e34f8"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# cross validation\n",
        "X_train_new = X_train[:int(len(X_train)*0.8), :]\n",
        "y_train_new = y_train[:int(len(y_train)*0.8)]\n",
        "X_validation = X_train[int(len(X_train)*0.8):, :]\n",
        "y_validation = y_train[int(len(y_train)*0.8):]\n",
        "print(X_train_new.shape, y_train_new.shape, X_validation.shape, y_validation.shape)\n",
        "\n",
        "print('=====training=====')\n",
        "c_list = [0.0001, 0.1, 1.0, 10.0, 100.0, 1000000]\n",
        "#g_list = [0.000001, 0.001,0.01, 0.1, 1.0, 10.0]\n",
        "kernel_list = ['rbf', 'poly', 'linear', 'sigmoid']\n",
        "for k in kernel_list:\n",
        "  for c in c_list:\n",
        "    clf = SVC(C=c, kernel=k, gamma = 0.001)\n",
        "    clf.fit(X_train_new, y_train_new)\n",
        "    y_pred_validation = clf.predict(X_validation) #evaluating the performance on validation set\n",
        "      \n",
        "    print(k + \", C = \" + str(c)+ \" , training accuracy = \" + \n",
        "      str(accuracy_score(y_validation, y_pred_validation))) \n",
        "    #y_hat = clf.predict(X_test)\n",
        "      #print(\"test: \" + str(sklearn.metrics.accuracy_score(y_hat, y_test)))\n",
        "\n",
        "#test\n",
        "c = 100\n",
        "k = 'rbf'\n",
        "g = 0.001\n",
        "clf = SVC(C=c, kernel=k, gamma = g)\n",
        "clf.fit(X_train_new, y_train_new)\n",
        "y_hat = clf.predict(X_test)\n",
        "print('\\n')\n",
        "print('=====testing=====')\n",
        "print(k + \", C = \" + str(c) + \" , test accuracy = \" + \n",
        "      str(sklearn.metrics.accuracy_score(y_hat, y_test)))\n",
        "\n",
        "#explanation\n",
        "print('\\n')\n",
        "print('=====explanation=====')\n",
        "print('parameter chosen: rbf, C = 100, gamma = 0.001')\n",
        "print('When gamma is set by default (1/n_features), poly gives out the best performance'+'\\n'\n",
        "        +'But if we specify gamma to 0.001 (by experiements), rbf actually performs the best'+ '\\n'\n",
        "        + 'According to the experiment, the training accuracy is roughly the same for C=10, 100 and 100000 for rbf'+ '\\n'\n",
        "        + 'I choose C as 100 because a very large C may lead to overfit, '+'\\n'\n",
        "        +'and a small C may not be accurate enough if more data is added in the future.')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((1149, 64), (1149,), (288, 64), (288,))\n",
            "=====training=====\n",
            "rbf, C = 0.0001 , training accuracy = 0.09722222222222222\n",
            "rbf, C = 0.1 , training accuracy = 0.9618055555555556\n",
            "rbf, C = 1.0 , training accuracy = 0.9826388888888888\n",
            "rbf, C = 10.0 , training accuracy = 0.9861111111111112\n",
            "rbf, C = 100.0 , training accuracy = 0.9861111111111112\n",
            "rbf, C = 1000000 , training accuracy = 0.9861111111111112\n",
            "poly, C = 0.0001 , training accuracy = 0.09722222222222222\n",
            "poly, C = 0.1 , training accuracy = 0.9756944444444444\n",
            "poly, C = 1.0 , training accuracy = 0.9756944444444444\n",
            "poly, C = 10.0 , training accuracy = 0.9756944444444444\n",
            "poly, C = 100.0 , training accuracy = 0.9756944444444444\n",
            "poly, C = 1000000 , training accuracy = 0.9756944444444444\n",
            "linear, C = 0.0001 , training accuracy = 0.9618055555555556\n",
            "linear, C = 0.1 , training accuracy = 0.9722222222222222\n",
            "linear, C = 1.0 , training accuracy = 0.9722222222222222\n",
            "linear, C = 10.0 , training accuracy = 0.9722222222222222\n",
            "linear, C = 100.0 , training accuracy = 0.9722222222222222\n",
            "linear, C = 1000000 , training accuracy = 0.9722222222222222\n",
            "sigmoid, C = 0.0001 , training accuracy = 0.09722222222222222\n",
            "sigmoid, C = 0.1 , training accuracy = 0.10069444444444445\n",
            "sigmoid, C = 1.0 , training accuracy = 0.6631944444444444\n",
            "sigmoid, C = 10.0 , training accuracy = 0.5486111111111112\n",
            "sigmoid, C = 100.0 , training accuracy = 0.5451388888888888\n",
            "sigmoid, C = 1000000 , training accuracy = 0.5277777777777778\n",
            "\n",
            "\n",
            "=====testing=====\n",
            "rbf, C = 100 , test accuracy = 0.9611111111111111\n",
            "\n",
            "\n",
            "=====explanation=====\n",
            "parameter chosen: rbf, C = 100, gamma = 0.001\n",
            "When gamma is set by default (1/n_features), poly gives out the best performance\n",
            "But if we specify gamma to 0.001 (by experiements), rbf actually performs the best\n",
            "According to the experiment, the training accuracy is roughly the same for C=10, 100 and 100000 for rbf\n",
            "I choose C as 100 because a very large C may lead to overfit, \n",
            "and a small C may not be accurate enough if more data is added in the future.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}